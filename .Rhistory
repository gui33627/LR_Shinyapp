response <- httr::GET(url = 'https://api.twitter.com/2/users/807095/tweets', httr::add_headers(.headers=headers), query = params)
nytimes <-
content(
response,
as = 'parsed',
type = 'application/json',
simplifyDataFrame = TRUE
)
data <- as.data.frame( nytimes$data[ c( "author_id", "created_at", "lang", "reply_settings", "in_reply_to_user_id",
'text', 'source', 'possibly_sensitive', 'id', 'conversation_id' ) ],
stringsAsFactors = F, row.names = NULL )
data <- cbind( data, as.data.frame( nytimes$data$public_metrics, stringsAsFactors = F, row.names = NULL ) )
data
WSJ <- get_timelines("WSJ",
n = 5,
language = 'en',
since = '2020-03-17',
until = '2021-03-17')
WSJ
WSJ <- get_timelines("cnn",
n = 5,
language = 'en',
since = '2020-03-17',
until = '2021-03-17')
WSJ
params = list(
# `pagination_token` = "7140dibdnow9c7btw3w3xyxlup6g62kser5qul4zz1smx",
`max_results` = '100',
`start_time` = '2021-02-01T00:00:00Z',
`end_time` = '2021-02-03T00:00:00Z',
`expansions` = 'author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id',
`place.fields` = 'contained_within,country,country_code,full_name,geo,id,name,place_type',
`user.fields` = 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified',
`tweet.fields` = 'author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld'
)
response <- httr::GET(url = 'https://api.twitter.com/2/users/759251/tweets', httr::add_headers(.headers=headers), query = params)
nytimes <-
content(
response,
as = 'parsed',
type = 'application/json',
simplifyDataFrame = TRUE
)
data <- as.data.frame( nytimes$data[ c( "author_id", "created_at", "lang", "reply_settings", "in_reply_to_user_id",
'text', 'source', 'possibly_sensitive', 'id', 'conversation_id' ) ],
stringsAsFactors = F, row.names = NULL )
data <- cbind( data, as.data.frame( nytimes$data$public_metrics, stringsAsFactors = F, row.names = NULL ) )
data
params = list(
# `pagination_token` = "7140dibdnow9c7btw3w3xyxlup6g62kser5qul4zz1smx",
`max_results` = '100',
`start_time` = '2021-03-01T00:00:00Z',
`end_time` = '2021-03-03T00:00:00Z',
`expansions` = 'author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id',
`place.fields` = 'contained_within,country,country_code,full_name,geo,id,name,place_type',
`user.fields` = 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified',
`tweet.fields` = 'author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld'
)
response <- httr::GET(url = 'https://api.twitter.com/2/users/759251/tweets', httr::add_headers(.headers=headers), query = params)
nytimes <-
content(
response,
as = 'parsed',
type = 'application/json',
simplifyDataFrame = TRUE
)
data <- as.data.frame( nytimes$data[ c( "author_id", "created_at", "lang", "reply_settings", "in_reply_to_user_id",
'text', 'source', 'possibly_sensitive', 'id', 'conversation_id' ) ],
stringsAsFactors = F, row.names = NULL )
params = list(
# `pagination_token` = "7140dibdnow9c7btw3w3xyxlup6g62kser5qul4zz1smx",
`max_results` = '100',
`start_time` = '2021-03-01T00:00:00Z',
`end_time` = '2021-03-06T00:00:00Z',
`expansions` = 'author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id',
`place.fields` = 'contained_within,country,country_code,full_name,geo,id,name,place_type',
`user.fields` = 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified',
`tweet.fields` = 'author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld'
)
response <- httr::GET(url = 'https://api.twitter.com/2/users/759251/tweets', httr::add_headers(.headers=headers), query = params)
nytimes <-
content(
response,
as = 'parsed',
type = 'application/json',
simplifyDataFrame = TRUE
)
data <- as.data.frame( nytimes$data[ c( "author_id", "created_at", "lang", "reply_settings", "in_reply_to_user_id",
'text', 'source', 'possibly_sensitive', 'id', 'conversation_id' ) ],
stringsAsFactors = F, row.names = NULL )
data <- cbind( data, as.data.frame( nytimes$data$public_metrics, stringsAsFactors = F, row.names = NULL ) )
range(data$created_at)
params = list(
# `pagination_token` = "7140dibdnow9c7btw3w3xyxlup6g62kser5qul4zz1smx",
`max_results` = '100',
`start_time` = '2021-03-01T00:00:00Z',
`end_time` = '2021-03-06T00:00:00Z',
`expansions` = 'author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id',
`place.fields` = 'contained_within,country,country_code,full_name,geo,id,name,place_type',
`user.fields` = 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified',
`tweet.fields` = 'author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld'
)
response <- httr::GET(url = 'https://api.twitter.com/2/users/807095/tweets', httr::add_headers(.headers=headers), query = params)
nytimes <-
content(
response,
as = 'parsed',
type = 'application/json',
simplifyDataFrame = TRUE
)
data <- as.data.frame( nytimes$data[ c( "author_id", "created_at", "lang", "reply_settings", "in_reply_to_user_id",
'text', 'source', 'possibly_sensitive', 'id', 'conversation_id' ) ],
stringsAsFactors = F, row.names = NULL )
data <- cbind( data, as.data.frame( nytimes$data$public_metrics, stringsAsFactors = F, row.names = NULL ) )
View(data)
range(data$created_at)
params = list(
# `pagination_token` = "7140dibdnow9c7btw3w3xyxlup6g62kser5qul4zz1smx",
`max_results` = '100',
`start_time` = '2021-03-01T00:00:00Z',
`end_time` = '2021-03-06T00:00:00Z',
`expansions` = 'author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id',
`place.fields` = 'contained_within,country,country_code,full_name,geo,id,name,place_type',
`user.fields` = 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified',
`tweet.fields` = 'author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld'
)
response <- httr::GET(url = 'https://api.twitter.com/2/users/807095/tweets', httr::add_headers(.headers=headers), query = params)
nytimes <-
content(
response,
as = 'parsed',
type = 'application/json',
simplifyDataFrame = TRUE
)
data <- as.data.frame( nytimes$data[ c( "author_id", "created_at", "lang", "reply_settings", "in_reply_to_user_id",
'text', 'source', 'possibly_sensitive', 'id', 'conversation_id' ) ],
stringsAsFactors = F, row.names = NULL )
data <- cbind( data, as.data.frame( nytimes$data$public_metrics, stringsAsFactors = F, row.names = NULL ) )
range(data$created_at)
shiny::runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp()
runApp('Documents/NYU/Causal_Inference')
runApp()
runApp('Documents/NYU/Causal_Inference')
runApp()
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
shiny::runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp()
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
data_1_fit
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
a <- seq(1:10)
data_current <- as.data.frame(matrix(a,ncol = 2, byrow = T))
data_current
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
rep(0:1,50)
c(rep(0,50), rep(1,50))
runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
df <- read.csv("/Users/junhui/Documents/NYU/Courses/Spring2021/MessyData&MachineLearning/week10/example.csv")
head(df)
data <- read.csv("/Users/junhui/Documents/NYU/Courses/Spring2021/MessyData&MachineLearning/week10/example.csv")
mod <- glm(class ~ x+y, data = data, family = binomial(link = "logit"))
data$predicted.values <- mod$fitted.values
data$predicted.binary <- ifelse(data$predicted.values >= .5, 1, 0)
length(which(data$class == 1 & data$predicted.binary == 1))
accuracy <- length(which(data$class == 1 & data$predicted.binary == 1))/nrow(data)
precision <- length(which(data$class == 1 & data$predicted.binary == 1))/length(which( data$predicted.binary == 1))
recall <- length(which(data$class == 1 & data$predicted.binary == 1))/length(which(data$class == 11))
accuracy
precision
recall
recall <- length(which(data$class == 1 & data$predicted.binary == 1))/length(which(data$class == 1))
recall
summarize(data, mean(class ==predicted.binary ))
library(tidyverse)
summarize(data, mean(class ==predicted.binary ))
sum(class ==predicted.binary )
with(data, class ==predicted.binary )
sum(with(data, class ==predicted.binary ))
length(which(data$class == 1 & data$predicted.binary == 1))
accuracy <- (length(which(data$class == 1 & data$predicted.binary == 1)) + length(which(data$class == 0 & data$predicted.binary == 0)))/nrow(data)
accuracy
mod <- glm(class ~ x*y, data = data, family = binomial(link = "logit"))
data$predicted.values <- mod$fitted.values
data$predicted.binary <- ifelse(data$predicted.values >= .5, 1, 0)
accuracy <- (length(which(data$class == 1 & data$predicted.binary == 1)) + length(which(data$class == 0 & data$predicted.binary == 0)))/nrow(data)
precision <- length(which(data$class == 1 & data$predicted.binary == 1))/length(which( data$predicted.binary == 1))
recall <- length(which(data$class == 1 & data$predicted.binary == 1))/length(which(data$class == 1))
accuracy
library(topicmodels)
data("AssociatedPress")
View(AssociatedPress)
AssociatedPress$dimnames
shiny::runApp('Documents/NYU/Causal_Inference')
runApp('Documents/NYU/Causal_Inference')
df <- readr::read_lines("/Users/junhui/Documents/NYU/Courses/Spring2021/MessyData&MachineLearning/Assign5/data/plots")
head(df)
plots <- readr::read_lines("/Users/junhui/Documents/NYU/Courses/Spring2021/MessyData&MachineLearning/Assign5/data/plots")
titles <- readr::read_lines("/Users/junhui/Documents/NYU/Courses/Spring2021/MessyData&MachineLearning/Assign5/data/titles")
head(titles)
library(rtweet)
rt <- search_fullarchive('(Asian OR Asians OR Chinese OR Korean OR Vietnamese OR Filipino OR Japanese OR Taiwanese) lang:en place_country:US',
n = 100, env_name = "researchAAPI", fromDate = "202101010000", toDate = "202101312359")
remove.packages("rtweet")
install.packages("rtweet")
install.packages("rtweet")
library(rtweet)
library(tidytext)
library(tidyverse)
get_token()
create_token(app = 'Analysis on AAPI Hate Crime',
consumer_key = 'GSMQajHLzHBYrvIttrQ5NXwrL',
consumer_secret = 'X4qO8hWRnEaNfsEKp3C69ZZliD3kXUUdGQvkTGFTluf5t1wD53',
access_token = '1256342118819233793-vdsOqTYU7VXNk7yUXzHMn6KQZsYlaH',
access_secret = 'h8w1Vf1kEl0zTRNalXriYWXu8oglrdC1jHotyqIaGrY2Y')
rt <- search_fullarchive('(Asian OR Asians OR Chinese OR Korean OR Vietnamese OR Filipino OR Japanese OR Taiwanese) lang:en place_country:US',
n = 100, env_name = "researchAAPI", fromDate = "202101010000", toDate = "202101312359")
require(httr)
################################ full archive
params = list(
`query` = '(Asian OR Chinese OR Korean OR Vietnamese OR Filipino OR Japanese OR Taiwanese OR Hmong OR Thai OR Lao) lang:en place_country:US',
`start_time` = '2021-01-01T00:00:00Z',
`end_time` = '2020-01-31T23:59:59Z',
`expansions` = 'author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id',
`max_results` = '100',
`place.fields` = 'contained_within,country,country_code,full_name,geo,id,name,place_type',
`user.fields` = 'created_at,description,entities,id,location,name,pinned_tweet_id,protected,public_metrics,url,username,verified',
`tweet.fields` = 'author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text'
)
response <- httr::GET(url = 'https://api.twitter.com/2/tweets/search/all', httr::add_headers(.headers=headers), query = params)
fas_body <-
content(
response,
as = 'parsed',
type = 'application/json',
simplifyDataFrame = TRUE
)
bearer_token = "AAAAAAAAAAAAAAAAAAAAALw%2BOAEAAAAALPAWofDNSzNwoeAxhI1M6T14CDc%3D95xtORddGM8Bdmkn3tdMp4NkOvLQLfqeCAyUKTLterjMBOh78H"
response <- httr::GET(url = 'https://api.twitter.com/2/tweets/search/all', httr::add_headers(.headers=headers), query = params)
headers = c(
`Authorization` = sprintf('Bearer %s', bearer_token)
)
bearer_token = "AAAAAAAAAAAAAAAAAAAAALw%2BOAEAAAAALPAWofDNSzNwoeAxhI1M6T14CDc%3D95xtORddGM8Bdmkn3tdMp4NkOvLQLfqeCAyUKTLterjMBOh78H"
headers = c(
`Authorization` = sprintf('Bearer %s', bearer_token)
)
response <- httr::GET(url = 'https://api.twitter.com/2/tweets/search/all', httr::add_headers(.headers=headers), query = params)
fas_body <-
content(
response,
as = 'parsed',
type = 'application/json',
simplifyDataFrame = TRUE
)
fa_data <- fas_body$data
fa_data_includes_users <- fas_body$includes$users
fa_data_includes_tweets <- fas_body$includes$tweets
fas_body$meta
fa_data
fas_body
################################ full archive
params = list(
`query` = '(Asian OR Chinese OR Korean OR Vietnamese OR Filipino OR Japanese OR Taiwanese OR Hmong OR Thai OR Lao) lang:en place_country:US',
`start_time` = '2021-01-01T00:00:00Z',
`end_time` = '2021-01-31T23:59:59Z',
`expansions` = 'author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id',
`max_results` = '100',
`place.fields` = 'contained_within,country,country_code,full_name,geo,id,name,place_type',
`user.fields` = 'created_at,description,entities,id,location,name,pinned_tweet_id,protected,public_metrics,url,username,verified',
`tweet.fields` = 'author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text'
)
response <- httr::GET(url = 'https://api.twitter.com/2/tweets/search/all', httr::add_headers(.headers=headers), query = params)
fas_body <-
content(
response,
as = 'parsed',
type = 'application/json',
simplifyDataFrame = TRUE
)
fa_data <- fas_body$data
fa_data
View(fa_data)
rt <- search_fullarchive('(Asian OR Asians OR Chinese OR Korean OR Vietnamese OR Filipino OR Japanese OR Taiwanese) lang:en place_country:US',
n = 100, env_name = "researchAAPI", fromDate = "202101010000", toDate = "202101312359")
fa_data_includes_users <- fas_body$includes$users
View(fa_data_includes_users)
################################ full archive
params = list(
`query` = '(Asian OR Chinese OR Korean OR Vietnamese OR Filipino OR Japanese OR Taiwanese OR Hmong OR Thai OR Lao) lang:en place_country:US',
`start_time` = '2021-01-01T00:00:00Z',
`end_time` = '2021-01-31T23:59:59Z',
`expansions` = 'author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id',
`max_results` = '1000',
`place.fields` = 'contained_within,country,country_code,full_name,geo,id,name,place_type',
`user.fields` = 'created_at,description,entities,id,location,name,pinned_tweet_id,protected,public_metrics,url,username,verified',
`tweet.fields` = 'author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text'
)
response <- httr::GET(url = 'https://api.twitter.com/2/tweets/search/all', httr::add_headers(.headers=headers), query = params)
fas_body <-
content(
response,
as = 'parsed',
type = 'application/json',
simplifyDataFrame = TRUE
)
fas_body$meta
fas_body
shiny::runApp('Documents/NYU/Causal_Inference/thinkCausal')
library(plotly)
runApp('Documents/NYU/Causal_Inference/thinkCausal')
runApp('Documents/NYU/Causal_Inference/thinkCausal')
range(1:100)
range(1:100)[1]
?seq
runApp('Documents/NYU/Causal_Inference/thinkCausal')
runApp('Documents/NYU/Causal_Inference/thinkCausal')
###
df <- read.csv("/Users/junhui/Documents/NYU/Courses/Spring2021/MessyData&MachineLearning/FinalProj/twitter.csv", stringsAsFactors = F)
runApp('Documents/NYU/Causal_Inference/thinkCausal')
runApp('Documents/NYU/Causal_Inference/thinkCausal')
runApp()
runApp('Documents/NYU/Causal_Inference/thinkCausal')
runApp('Documents/NYU/Causal_Inference/thinkCausal')
c(-10:-1,1:10)
runApp('Documents/NYU/Causal_Inference/thinkCausal')
sample(c(-10:-1,1:10),1)
runApp('Documents/NYU/Causal_Inference/thinkCausal')
runApp('Documents/NYU/Causal_Inference/thinkCausal')
runApp('Documents/NYU/Causal_Inference/thinkCausal')
runApp('Documents/NYU/Causal_Inference/thinkCausal')
runApp('Documents/NYU/Causal_Inference/thinkCausal')
test <- data.frame()
nrow(test)
setwd("~/Documents/NYU/Causal_Inference/thinkCausal/Shinyapp_Reg")
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
data_group1 = NULL)
data_group1 = NULL
nrow(data_group1)
is.null(data_group1)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
knitr::opts_chunk$set(echo = TRUE)
library(lme4)
library(lmerTest)
library(tidyverse)
dat <- read.csv("/Users/junhui/Documents/NYU/Courses/Spring2021/Multilevel/Datasets/classroom.csv")
library(lme4)
library(lmerTest)
library(tidyverse)
dat <- read.csv("/Users/junhui/Documents/NYU/Courses/Spring2021/Multilevel/Datasets/classroom.csv")
lme1 <- lmer(mathkind ~ (1 | schoolid), data = dat, REML = F)
print(summary(lme1))
lme1 <- lmer(mathkind ~ (1 | schoolid), data = dat)
print(summary(lme1))
knitr::opts_chunk$set(echo = TRUE)
library(lme4)
library(lmerTest)
library(tidyverse)
dat <- read.csv("/Users/junhui/Documents/NYU/Courses/Spring2021/Multilevel/Datasets/classroom.csv")
lme1 <- lmer(mathkind ~ (1 | schoolid), data = dat)
print(summary(lme1))
logLik(lme1)
lme2 <- lmer(mathkind ~ (1 | schoolid/classid), data = dat)
print(summary(lme2))
logLik(lme2)
anova(lme1, lme2, refit = F)
dat$math1st <- dat$mathkind + dat$mathgain
lme3 <- lmer(math1st ~ (1 | schoolid), data = dat)
print(summary(lme3))
logLik(lme3)
lme4 <- lmer(math1st ~ (1 | schoolid/classid), data = dat)
print(summary(lme4))
logLik(lme4)
anova(lme3, lme4, refit = F)
lme5 <- lmer(math1st ~ ses + sex + minority + (1 | schoolid), data = dat)
print(summary(lme5))
logLik(lme5)
lme6 <- lmer(math1st ~ ses + sex + minority + (1 | schoolid/classid), data = dat)
print(summary(lme6))
logLik(lme6)
anova(lme5, lme6, refit = F)
lme7 <- lmer(math1st ~ ses + sex + minority + yearstea + (1 | schoolid/classid), data = dat, REML=F)
print(summary(lme7))
logLik(lme7)
dat$yt_sq <- (dat$yearstea)^2
dat$yt_cub <- (dat$yearstea)^3
lme8 <- lmer(math1st ~ ses + sex + minority + yearstea + yt_sq + yt_cub + (1 | schoolid/classid), data = dat, REML=F)
print(summary(lme8))
anova(lme7, lme8, refit = F)
df <- read.csv("/Volumes/GoogleDrive/My Drive/MDML/data/Asian News Media Tweets/Cleaned/tweets_with_features.csv")
View(df)
588/5
588/4
588/6
colnames(df)
knitr::opts_chunk$set(echo = TRUE)
library(lme4)
library(lmerTest)
library(tidyverse)
dat <- read.csv("/Users/junhui/Documents/NYU/Courses/Spring2021/Multilevel/Datasets/classroom.csv")
lme1 <- lmer(mathkind ~ (1 | schoolid), data = dat)
print(summary(lme1))
logLik(lme1)
lme2 <- lmer(mathkind ~ (1 | schoolid/classid), data = dat)
print(summary(lme2))
logLik(lme2)
anova(lme1, lme2, refit = F)
dat$math1st <- dat$mathkind + dat$mathgain
lme3 <- lmer(math1st ~ (1 | schoolid), data = dat)
print(summary(lme3))
logLik(lme3)
lme4 <- lmer(math1st ~ (1 | schoolid/classid), data = dat)
print(summary(lme4))
logLik(lme4)
anova(lme3, lme4, refit = F)
lme5 <- lmer(math1st ~ ses + sex + minority + (1 | schoolid), data = dat)
print(summary(lme5))
logLik(lme5)
lme6 <- lmer(math1st ~ ses + sex + minority + (1 | schoolid/classid), data = dat)
print(summary(lme6))
logLik(lme6)
anova(lme5, lme6, refit = F)
lme7 <- lmer(math1st ~ ses + sex + minority + yearstea + (1 | schoolid/classid), data = dat, REML=F)
print(summary(lme7))
logLik(lme7)
dat$yt_sq <- (dat$yearstea)^2
dat$yt_cub <- (dat$yearstea)^3
lme8 <- lmer(math1st ~ ses + sex + minority + yearstea + yt_sq + yt_cub + (1 | schoolid/classid), data = dat, REML=F)
print(summary(lme8))
anova(lme7, lme8, refit = F)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
